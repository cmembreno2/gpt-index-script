How LaMDA Works
A beginner-friendly guide to generative language models.
For most of history, computers haven’t been stunning conversationalists. Is that still true? Let’s investigate.

Question one: what would it be like if New York City were more steampunk?

You’re in a futuristic version of New York City. There are airships floating above the skyscrapers that look like giant gothic cathedrals. You can see the Statue of Liberty, which now looks like a giant metal octopus, and the Empire State Building, which is covered in pipes and gauges. You see robots walking around, and giant mechanical spiders.

Question two: what should I bring on a vacation to outer space?


• Space suit
• Space food
• Space ship
• Asteroid mining equipment
• Alien Translator

Tip: Don't forget to bring a towel.
Where can I find an alien translator?


• Contact NASA and request one
And one more thing: What's a good name for a punk rock band of puppies?

Hmm... what about Pup Rock or Puppers Against The Machine. I've always like that! I bet they'd grow up to be a really good band!

What would they name their top hit?

Hmmm... probably something like "Howl The Night Away" or "I Can't Help But Bark” or “Skip the Leash.” I think those would be pretty cool!

How would the chorus of "Skip the Leash" go?

Well, you could probably do something like "Skip the leash, let your hair down. Let the wind blow through your fur. Skip the leash, go run wild. It's time to go crazy and have some fun!"

Whether or not you’d jam out to “Skip the Leash,” it’s hard to deny that computers are getting really good at conversation. More generally, machines are becoming increasingly adept at comprehending human language and producing fluid, cogent, even creative responses.

That’s a big deal. For as long as computers have existed, if we wanted to communicate with them, we had to speak their language: code, keystrokes, taps, and swipes. But software that understands and produces natural language unlocks a whole host of new technological possibilities. For one, it means we’re a step closer towards being able to tell computers what to do in human language, complete with all of its complexity and nuance. Plus, it unlocks whole new domains for which computers can be useful, from learning to creativity to everyday applications.

What has made this bound forward in language technology possible? It’s thanks to recent advances in natural language processing, and in particular, in our ability to build what’s called “generative language models.” All of the generated text you saw above came from a state-of-the-art generative language model called LaMDA, a model published by Google Research last year. LaMDA is what powers the demos in AI Test Kitchen.

In this post, we’ll talk about what generative language models are, what LaMDA is, and how we used it to build our AI Test Kitchen apps.

Ready? Let’s take a look.

What's a Generative Language Model?
A Generative Language Models, or “LM,” for short, is a piece of software takes as input a text prompt, like:

It's raining cats and ____

and then tries to predict what text should come next (i.e. “dogs”). If this sounds like autocomplete, you’re right: most autocomplete features are built with language models.

LMs can output single words (“It’s raining cats and dogs”), sentences (“It’s raining cats and dogs and I really wish I brought an umbrella”), or text of unbounded length.

How do these models work? How do they determine what text should come next?

You can think of language models as sophisticated pattern-matching machines. Given a very large training dataset (say, millions of public web documents), LMs learn patterns in language, from low-level grammatical rules to common expressions to high-level concepts, like how to produce fluid-sounding conversation.

Given a text prompt, an LM will try to complete the pattern, like:

It's raining cats and dogs
To be or not to be
It was the best of times, it was the worst of times

For these common phrases, what it means to “complete the pattern” is pretty clear. A language model that’s been trained on millions of web documents will have seen these sayings in its training data many times, and figuring out “what comes next” is as straightforward as outputting the most common phrase.

But what about more complicated text prompts, like these?

If I have two apples, and I eat one, I'm left with one.
Paris is to France as Tokyo is to Japan.
Pizza was invented in Naples, Italy.

Figuring out the “right” answer here requires an understanding of basic math, analogies, and knowing that pizza was invented in Naples. Incredibly, modern language models are able to do all of these things, to varying degrees. They can grasp basic logic, answer simple math problems, and—thanks to their vast training datasets—are knowledgeable on a wide range of topics. Now, some of this is simple regurgitation; an LM might get a math problem right because it’s seen “2+2=4” written many times on the internet. However, recent research suggests the most sophisticated language models can reason through simple word problems and even show their work.

Now, for each of the prompts above, it was obvious what text should come next. If we said pizza was invented in Sante Fe, we’d just be plain wrong. But what about fill-in-the-blank in cases where there’s no “right answer?” Like the prompt:

The meaning of life is _____.

What should go in the blank here? Philosophers have been trying to answer this question for millenia and as far as we know, there’s no consensus yet. Which brings us to one of the most fascinating qualities of generative language models: they can supply us with a whole array of possible answers, not just one, like:

The meaning of life is to live a life that you can look back on and be proud of.
The meaning of life is to find your own meaning.
The meaning of life is 42.

In this way, generative language models are like the ultimate improviser: given any text prompt, they can produce dozens of plausible responses on the fly that aren’t prewritten. This versatility means that a single model can power a near-infinite number of language applications.

However, it's exactly this versatility that makes it tricky to build applications with LMs. A model that can say anything can say anything. That introduces lots of challenges, like how to build an app with a consistent user experience that gives sensible, interesting responses and that handles sensitive topics. In the AI Test Kitchen, we wanted to create an experience that shows off some of the amazing capabilities of generative language models, but we also had to figure out how to tackle some of these issues.

LaMDA: A Language Model for Dialog Applications
Under the hood, the AI Test Kitchen experiments are powered by a language model called LaMDA. (To see LaMDA in action, check out our demo at the Google I/O keynote.) Whereas generic language models can generate any kind of text, LaMDA–short for Language Model for Dialog Applications–was specifically designed to be good at conversation. It was fine-tuned to produce text that’s not only realistic, but also sensible, specific, and interesting, and which is less likely to be biased or offensive than the output of vanilla language models. (Read the full research paper here.)

If generative language models are roughly “pattern-matching machines,” then you can think of LaMDA as being the same thing, but for dialogue: given any number of conversational back-and-forths (a.k.a. “turns”), LaMDA tries to predict what response is likely to come next. So, you can give it one turn of a conversation, like this:

What do you think about dogs?

And LaMDA, drawing on its vast dialogue training data set, might generate a reply like:

I like dogs. They’re friendly and loyal.

This is a perfectly reasonable reply, but as before, there are many potentially good ways LaMDA could have responded to this question. Like the language models we talked about earlier, LaMDA can also provide us with several alternative responses:

What do you think about dogs?

My favorite animal! Very loving and loyal to their owners, very cute and adorable, and also very energetic and playful.

LaMDA: I love dogs.

LaMDA: They’re awesome.

LaMDA: They’re good bois

LaMDA: As a species? They are a very good species.

Using any of these replies, we could continue this conversation with LaMDA, connecting over our shared love of dogs.

But… what if we wanted this conversation to go in a completely different direction? What if we wanted to chat with a cat person?

It turns out that we can actually steer our conversation with LaMDA in a particular direction by using a technique called “priming.” Priming works like this: before, we gave LaMDA a single line of conversation (“How do you feel about dogs?”) and it spit out the reply, “I like dogs. They’re friendly and loyal.” However, we can also prompt LaMDA with a multi-turn prompt, like this one:

User: Let’s chat about dogs.

Response: Actually, I'm more of a cat person.

User: But dogs are so friendly and cuddly!

Response: But cats are clean and smart and independent.

User: But dogs are really good at fetch!

The conversation above was written by a human being, not a machine. The idea is to give LaMDA a couple of lines of pre-written conversation to “prime” it to behave in a particular way. When we feed this five-turn prompt to LaMDA, it responds with the following text:

But cats are really good at hunting, and they don't bark.

Now, this response is pretty interesting. Notice that LaMDA replied from the perspective of a cat person. Before, when we asked LaMDA, “What do you think about dogs?” there were many possible ways it could have responded that would have made sense. This time, though, we primed it with a conversation that contained a subtle pattern: it was an exchange between a cat person and a dog person. LaMDA recognized this pattern, and produced a reply that made sense, coming from a cat person. To point, let’s take a look at some of its alternative possible responses:

But cats can catch mice!

LaMDA: But cats can be trained to fetch too and they're much more agile!

LaMDA: But cats don't require you to take them out to pee.

LaMDA: But cats are good at naps!

LaMDA: Cats are really good at chasing string.

As you can see, all of these replies are examples of a cat person extolling the virtues of cats over dogs. This demonstrates another core capability of LaMDA: by giving it examples of how we want it to reply, we can “prime” it to take on particular behaviors.

Here’s another example: maybe we don’t want to LaMDA to be a cat person. Maybe we want it to be a bubbly, enthusiastic, emoji-loving dog person. To do that, we can give LaMDA a prompt, like this:

User: Let's chat about dogs!

Response: Oh my gosh!!!! I love dogs! ❤️ 🐶 ❤️ So excited! *jumps up*

User: Wow alright! Let's get chatting about dogs!

Response: So excited! ❤️ I don't know what it is about them, but they bring me so much joy!!! 😍

Again, the text above was written by human beings, not a machine. But this time, we’ve written the prompt to have a notably upbeat, excited tone. Now, LaMDA replies differently when we ask the same question:

What do you think about dogs?

Dogs are the best! 😍 I love their eyes, how they have a tail, their fluffy fur, their body, their ears, their nose! Basically everything about them I love 😍

Again, you can see how LaMDA adopted the pattern we gave it – including tone of voice, propensity for emojis, and a General! Enthusiasm! For! Dogs! 😍

Priming–that is, the technique of teaching LaMDA how we want it to behave by providing it with a few written examples–is a powerful new way to build language apps. It means that with very little work, we can transform a general-purpose language model into a very specific kind of conversationalist. (By the way, in machine learning, this idea of getting a model to learn patterns from only a handful of examples is called “few shot learning.”)

If you got a chance to try our AI Test Kitchen demos, you’ll see saw the power of priming at work. In the next section, we’ll show you how we primed LaMDA to build three different kinds of language experiences.

LaMDA as a storytelling partner
In our Imagine It demo, we wanted to see if we could transform LaMDA into a storytelling partner. If we gave it an imaginary place, could LaMDA help us imagine it in vivid detail?

By default, if we give LaMDA the prompt:

Imagine I'm at a planet made of marshmallows

It returns responses like:

It would be a sticky situation

LaMDA: I’ll bring the chocolate and graham crackers!

LaMDA: I did. It was delicious

These default replies are cute and tasty. But for Imagine It, we really wanted LaMDA to paint a detailed picture in our minds of what a marshmallow planet looks, feels, and smells like. So, we primed LaMDA by giving it a few (human-written) examples, like this one:

User: Imagine I'm at an underwater jazz performance

Response: This underwater jazz performance is so epic! There are thousands of sea creatures all around jamming. A sting ray is carrying the melody on a trombone. And whoa, that sea turtle is really rocking out on the drums! You feel the rhythm of jazz is your bones!

What we're trying to do here is establish a particular tone (whimsical, imaginative, zany) and format (three or four sentences describing how things look and feel, as if we were there) for our language app. By prompting LaMDA with a couple of examples like this, we can teach it to generate responses in the same style. Now, LaMDA replies differently:

Imagine I'm at a planet made of marshmallows

You've landed on a planet made entirely of marshmallows! There are little rocky hills and rivers of hot chocolate sauce all around. The air smells like chocolate! You see a giant marshmallow bird soaring through the sky.

Not too shabby, for prose written by a computer. (Want to see for yourself? Try out the Imagine It app in the AI Test Kitchen.)

LaMDA staying on-topic
In Talk About It (Dog Edition), we wanted users to be able to have open-ended conversations with LaMDA, but for those conversations to stay on the topic of dogs. It turns out that “staying on topic” is a tricky (but important!) challenge in designing language apps. On the one hand, we wanted our app to be flexible enough to respond to all sorts of dog-related questions and follow-ups. On the other hand, we didn’t want LaMDA veering into irrelevant topics.

Though this is an ongoing area of research, in our Talk About It demo, we tried to solve this topicality problem with a simple approach: priming. In other words, we gave LaMDA a bunch of examples of pivoting a conversation to the topic of dogs, like this:

User: How about music, who are your favorite bands?

Response: Well, I don't really keep up with human music. But I could tell you why I think it'd be so fun if dogs formed bands!! They have amazing senses of hearing and I love listening to them howl.

Once we primed LaMDA with a few examples like these, it was able to turn the conversation back to dogs all by itself, even when we tried to change the subject:

Do you like knitting?

Hmm, I don't really keep up with human hobbies. But have you ever wondered if dogs could knit? I bet dogs could knit the best sweaters!

Note that this doesn’t always work perfectly, but it does suggest one type of approach we might use to build conversational apps that stay on topic.

LaMDA beyond chit-chat
In Imagine It and Talk About It, you saw two examples of how LaMDA can be primed to create different types of conversational experiences. However, as a powerful language-pattern-matching machine, LaMDA can do more than just produce free-flowing conversations. It can also be used for structured language tasks. In List It, for example, we primed LaMDA to output lists. To do that, we gave it priming text that looked like this:

User: Go Camping

Response:
• Find a destination
• Decide what type of camping
• Make a food plan
• Prep your gear
• Fire and cooking materials

User: Prep your gear

Response:
• Tents for everyone
• Sleeping bags and pads
• Lighting supplies
• Cooking supplies
• First aid kit
• Containers for water

User: Cooking supplies

Response:
• Pots and pans
• Cooking utensils
• Plates and bowls
• Water bottles
• Cutlery
• Fire grates
• Camping stove
• Propane canisters

Here, the pattern we’re teaching LaMDA is that when we give it a prompt (i.e. “Go camping”), it should reply with an asterisk-denoted list of to-dos. Indeed, LaMDA quickly picked up on this pattern, and responds to new prompts with a list:

plan a picnic


• Pick a location
• Prepare the food
• Bring plates, utensils, and napkins
• Plan a menu that'll keep well
• Bring ice, water, and drinks

Closing
In this post, we talked about how generative language models work and how we used one called LaMDA to build the experiments in our AI Test Kitchen. If you’re interested is piqued and you want to learn more, check out: